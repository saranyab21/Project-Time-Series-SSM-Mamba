{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87af55ec-67a4-45bc-9fca-872c09414ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 0) Imports & paths (project-structure aware) =========\n",
    "import os, re, zipfile, warnings, glob\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from tsfresh.feature_extraction import extract_features, EfficientFCParameters\n",
    "\n",
    "# --- Paths based on your folder structure shown in screenshots ---\n",
    "# This notebook lives in: .../gait-mamba/src\n",
    "SRC_DIR   = Path.cwd()\n",
    "ROOT_DIR  = SRC_DIR.parent                  # .../gait-mamba\n",
    "DATA_DIR  = ROOT_DIR / \"data\"\n",
    "RAW_DIR   = DATA_DIR / \"raw\" / \"gait-in-parkinsons-disease-1.0.0\"\n",
    "PROC_DIR  = DATA_DIR / \"processed\"\n",
    "FIG_DIR   = ROOT_DIR / \"reports\" / \"figs\"\n",
    "\n",
    "for d in [RAW_DIR, PROC_DIR, FIG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Try to find the ZIP in a few sensible places (optional; only used if RAW_DIR missing)\n",
    "ZIP_CANDIDATES = [\n",
    "    SRC_DIR / \"gait-in-parkinsons-disease-1.0.0.zip\",\n",
    "    ROOT_DIR / \"gait-in-parkinsons-disease-1.0.0.zip\",\n",
    "    DATA_DIR / \"raw\" / \"gait-in-parkinsons-disease-1.0.0.zip\",\n",
    "    Path(\"gait-in-parkinsons-disease-1.0.0.zip\"),\n",
    "]\n",
    "ZIP_PATH = next((z for z in ZIP_CANDIDATES if z.exists()), None)\n",
    "\n",
    "SEED = 42\n",
    "N_JOBS_TSFRESH = max(1, os.cpu_count() or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "701d1497-b2a4-41c7-9d5c-02a7de0f0502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data at: C:\\Users\\admin\\OneDrive\\Desktop\\SEM-2-3\\Project-Time Series\\gait-mamba\\data\\raw\\gait-in-parkinsons-disease-1.0.0\n",
      "Total .txt files found: 309 | Recognized trial files: 306\n",
      "Sample trials:\n",
      "- GaCo01_01.txt\n",
      "- JuCo01_01.txt\n",
      "- JuPt01_01.txt\n",
      "- SiCo01_01.txt\n",
      "- JuPt01_02.txt\n",
      "- JuPt01_03.txt\n",
      "- JuPt01_04.txt\n",
      "- JuPt01_05.txt\n",
      "Sliding window: 3 samples (~30 ms), step: 2 (~15 ms) at 100 Hz\n",
      "\n",
      "Loaded demographics columns: ['subject_id', 'sex', 'age', 'label']\n",
      "subjects: 40\n",
      "label counts: {1.0: 40}\n",
      "\n",
      "Dataset summary -> Patients: 40 | Controls: 0 | Males: 26 | Females: 14\n",
      "Age: mean=70.1, std=7.9, min=53, max=82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_21384\\3208810506.py:58: UserWarning: Could not detect sampling rate; defaulting to 100 Hz\n",
      "  warnings.warn(\"Could not detect sampling rate; defaulting to 100 Hz\")\n"
     ]
    }
   ],
   "source": [
    "# ========= 1) Unpack zip (idempotent; only if raw folder not present) =========\n",
    "if not RAW_DIR.exists():\n",
    "    if ZIP_PATH is None:\n",
    "        raise FileNotFoundError(\n",
    "            \"Raw folder not found and ZIP not located. \"\n",
    "            \"Place 'gait-in-parkinsons-disease-1.0.0.zip' under project root or data/raw.\"\n",
    "        )\n",
    "    with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
    "        z.extractall(DATA_DIR / \"raw\")\n",
    "    if not RAW_DIR.exists():\n",
    "        RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        for p in (DATA_DIR / \"raw\").glob(\"*\"):\n",
    "            if p.is_file():\n",
    "                p.rename(RAW_DIR / p.name)\n",
    "\n",
    "print(f\"Data at: {RAW_DIR}\")\n",
    "\n",
    "# ========= 2) Discover trials & parse ids =========\n",
    "TRIAL_RE = re.compile(r\"^(?P<prefix>[A-Za-z]+)(?P<sid>\\d{2})_(?P<tid>\\d{2})\\.txt$\")\n",
    "\n",
    "all_txt = sorted(glob.glob(str(RAW_DIR / \"**\" / \"*.txt\"), recursive=True))\n",
    "names   = [os.path.basename(p) for p in all_txt]\n",
    "matches = [(p, TRIAL_RE.match(os.path.basename(p))) for p in all_txt]\n",
    "trial_paths = [(p, m) for p, m in matches if m is not None]\n",
    "\n",
    "def prefix_to_label(prefix: str) -> int:\n",
    "    up = prefix.upper()\n",
    "    if \"PT\" in up: return 1\n",
    "    if \"CO\" in up: return 0\n",
    "    return -1\n",
    "\n",
    "trials = []\n",
    "for path, m in trial_paths:\n",
    "    pr  = m.group(\"prefix\")\n",
    "    sid = m.group(\"sid\")\n",
    "    tid = m.group(\"tid\")\n",
    "    label = prefix_to_label(pr)\n",
    "    trials.append({\n",
    "        \"filepath\": path,\n",
    "        \"filename\": os.path.basename(path),\n",
    "        \"prefix\": pr, \"subject_id\": sid, \"trial_id\": tid,\n",
    "        \"label_from_name\": label\n",
    "    })\n",
    "trials = pd.DataFrame(trials).sort_values([\"subject_id\",\"trial_id\"]).reset_index(drop=True)\n",
    "print(f\"Total .txt files found: {len(all_txt)} | Recognized trial files: {len(trials)}\")\n",
    "print(\"Sample trials:\", *trials['filename'].head(8).tolist(), sep=\"\\n- \")\n",
    "\n",
    "# ========= 3) Sampling rate detection =========\n",
    "def detect_fs(raw_dir: Path) -> int:\n",
    "    fmt = raw_dir / \"format\"\n",
    "    if fmt.exists():\n",
    "        txt = fmt.read_text(errors=\"ignore\")\n",
    "        m = re.search(r\"(?i)(sampling\\s*frequency|fs)\\D+(\\d+)\", txt)\n",
    "        if m:\n",
    "            fs = int(m.group(2))\n",
    "            print(f\"Detected sampling rate from 'format': {fs} Hz\")\n",
    "            return fs\n",
    "    warnings.warn(\"Could not detect sampling rate; defaulting to 100 Hz\")\n",
    "    return 100\n",
    "\n",
    "FS = detect_fs(RAW_DIR)\n",
    "\n",
    "# Convert 30 ms window & 15 ms step to samples\n",
    "WIN_MS, STEP_MS = 30, 15\n",
    "win_samples  = max(1, round(FS * (WIN_MS/1000)))\n",
    "step_samples = max(1, round(FS * (STEP_MS/1000)))\n",
    "print(f\"Sliding window: {win_samples} samples (~{WIN_MS} ms), step: {step_samples} (~{STEP_MS} ms) at {FS} Hz\")\n",
    "\n",
    "# ========= 4) Load demographics (robust) =========\n",
    "def load_demographics(raw_dir: Path) -> pd.DataFrame:\n",
    "    xls  = raw_dir / \"demographics.xls\"\n",
    "    html = raw_dir / \"demographics\"\n",
    "    if xls.exists():\n",
    "        df = pd.read_excel(xls)\n",
    "    elif html.exists():\n",
    "        df = pd.read_html(html.read_text(errors=\"ignore\"))[0]\n",
    "    else:\n",
    "        # still proceed w/out demographics; labels will come from filenames\n",
    "        return pd.DataFrame(columns=[\"subject_id\",\"sex\",\"age\",\"label\"])\n",
    "\n",
    "    df.columns = [str(c).strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    cand_sid = [c for c in df.columns if c in (\"subject\",\"subject_id\",\"subj\",\"patient\",\"code\",\"id\")]\n",
    "    if not cand_sid:\n",
    "        return pd.DataFrame(columns=[\"subject_id\",\"sex\",\"age\",\"label\"])\n",
    "    sid_col = cand_sid[0]\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out[\"subject_id\"] = (\n",
    "        df[sid_col]\n",
    "        .astype(str)\n",
    "        .str.extract(r\"(\\d+)\")\n",
    "        .iloc[:,0]\n",
    "        .astype(int).astype(str).str.zfill(2)\n",
    "    )\n",
    "    out[\"sex\"] = (df[\"sex\"] if \"sex\" in df.columns else df.get(\"gender\",\"unknown\")).astype(str).str.lower()\n",
    "    out[\"age\"] = pd.to_numeric(df[\"age\"], errors=\"coerce\") if \"age\" in df.columns else np.nan\n",
    "\n",
    "    lbl_col = next((c for c in (\"label\",\"group\",\"diagnosis\",\"status\") if c in df.columns), None)\n",
    "    if lbl_col is not None:\n",
    "        g = df[lbl_col].astype(str).str.upper()\n",
    "        out[\"label\"] = np.where(g.str.contains(\"PD\"), 1,\n",
    "                         np.where(g.str.contains(\"CO\")|g.str.contains(\"CONTROL\")|g.str.contains(\"HEALTH\"), 0, np.nan))\n",
    "    else:\n",
    "        out[\"label\"] = np.nan\n",
    "    return out.drop_duplicates(subset=\"subject_id\").reset_index(drop=True)\n",
    "\n",
    "demo = load_demographics(RAW_DIR)\n",
    "\n",
    "# derive label from filenames (per-subject majority of PT/CO) if demographics missing\n",
    "name_label = trials.groupby(\"subject_id\")[\"label_from_name\"] \\\n",
    "                   .agg(lambda s: s[s!=-1].mode().iloc[0] if (s!=-1).any() else np.nan)\n",
    "demo = demo.merge(name_label.rename(\"label_name\"), on=\"subject_id\", how=\"outer\")\n",
    "demo[\"label\"] = demo[\"label\"].fillna(demo[\"label_name\"]).astype(float)\n",
    "demo = demo.drop(columns=[\"label_name\"])\n",
    "\n",
    "print(\"\\nLoaded demographics columns:\", list(demo.columns))\n",
    "print(\"subjects:\", demo[\"subject_id\"].nunique())\n",
    "print(\"label counts:\", demo[\"label\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "def describe_dataset(demo_df: pd.DataFrame):\n",
    "    sex = demo_df.get(\"sex\",\"unknown\").astype(str).str.lower().str.extract(\"(male|female)\") \\\n",
    "                  .fillna(\"unknown\")[0]\n",
    "    males = int((sex==\"male\").sum()); females = int((sex==\"female\").sum())\n",
    "    ages = pd.to_numeric(demo_df.get(\"age\", np.nan), errors=\"coerce\")\n",
    "    n_pd = int((demo_df[\"label\"]==1).sum()); n_co = int((demo_df[\"label\"]==0).sum())\n",
    "    print(f\"\\nDataset summary -> Patients: {n_pd} | Controls: {n_co} | Males: {males} | Females: {females}\")\n",
    "    if ages.notna().any():\n",
    "        print(f\"Age: mean={ages.mean():.1f}, std={ages.std():.1f}, min={ages.min():.0f}, max={ages.max():.0f}\")\n",
    "describe_dataset(demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8351d50-7651-486d-a0c8-5856dd135107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSFresh: extracting features for 40 subjects (win=30ms, step=15ms, fs=100Hz, n_jobs=16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:   2%|█▍                                                        | 1/40 [28:31<18:32:31, 1711.58s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 1/40 done | last=1711.6s avg=1711.6s elapsed=28.5m ETA=1112.5m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:   5%|██▉                                                       | 2/40 [50:05<15:28:24, 1465.90s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 2/40 done | last=1293.9s avg=1502.7s elapsed=50.1m ETA=951.7m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:   8%|████▏                                                   | 3/40 [1:33:53<20:31:18, 1996.71s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 3/40 done | last=2628.3s avg=1877.9s elapsed=93.9m ETA=1158.1m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  10%|█████▌                                                  | 4/40 [2:12:39<21:15:50, 2126.39s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 4/40 done | last=2325.2s avg=1989.8s elapsed=132.7m ETA=1193.9m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  12%|███████                                                 | 5/40 [2:48:34<20:46:23, 2136.68s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 5/40 done | last=2154.9s avg=2022.8s elapsed=168.6m ETA=1180.0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  15%|████████▍                                               | 6/40 [3:37:51<22:49:00, 2415.88s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 6/40 done | last=2957.8s avg=2178.6s elapsed=217.9m ETA=1234.6m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  18%|█████████▊                                              | 7/40 [4:20:07<22:30:20, 2455.16s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 7/40 done | last=2536.0s avg=2229.7s elapsed=260.1m ETA=1226.3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  20%|███████████▏                                            | 8/40 [4:59:30<21:33:46, 2425.83s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 8/40 done | last=2363.0s avg=2246.4s elapsed=299.5m ETA=1198.1m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  22%|████████████▌                                           | 9/40 [6:00:33<24:13:05, 2812.44s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 9/40 done | last=3662.5s avg=2403.7s elapsed=360.6m ETA=1241.9m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  25%|█████████████▊                                         | 10/40 [6:55:09<24:37:47, 2955.57s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 10/40 done | last=3276.0s avg=2490.9s elapsed=415.2m ETA=1245.5m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  28%|███████████████▏                                       | 11/40 [7:56:50<25:38:43, 3183.57s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 11/40 done | last=3700.5s avg=2600.9s elapsed=476.8m ETA=1257.1m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  30%|████████████████▌                                      | 12/40 [8:36:18<22:49:59, 2935.70s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 12/40 done | last=2368.7s avg=2581.6s elapsed=516.3m ETA=1204.7m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  32%|█████████████████▉                                     | 13/40 [9:32:12<22:58:00, 3062.24s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 13/40 done | last=3353.4s avg=2640.9s elapsed=572.2m ETA=1188.4m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  35%|██████████████████▉                                   | 14/40 [10:23:03<22:05:33, 3059.00s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 14/40 done | last=3051.5s avg=2670.3s elapsed=623.1m ETA=1157.1m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  38%|████████████████████▎                                 | 15/40 [11:51:14<25:54:56, 3731.85s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 15/40 done | last=5291.2s avg=2845.0s elapsed=711.2m ETA=1185.4m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  40%|█████████████████████▌                                | 16/40 [12:48:17<24:15:33, 3638.90s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 16/40 done | last=3423.0s avg=2881.1s elapsed=768.3m ETA=1152.4m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  42%|██████████████████████▉                               | 17/40 [13:47:11<23:02:44, 3607.15s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 17/40 done | last=3533.3s avg=2919.5s elapsed=827.2m ETA=1119.1m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  45%|████████████████████████▎                             | 18/40 [14:36:17<20:49:49, 3408.60s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 18/40 done | last=2946.4s avg=2921.0s elapsed=876.3m ETA=1071.0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  48%|█████████████████████████▋                            | 19/40 [15:15:53<18:04:25, 3098.36s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 19/40 done | last=2375.6s avg=2892.3s elapsed=915.9m ETA=1012.3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  50%|███████████████████████████                           | 20/40 [16:24:23<18:53:59, 3402.00s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 20/40 done | last=4109.6s avg=2953.1s elapsed=984.4m ETA=984.4m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  52%|████████████████████████████▎                         | 21/40 [18:05:30<22:10:38, 4202.00s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 21/40 done | last=6067.1s avg=3101.4s elapsed=1085.5m ETA=982.1m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  55%|█████████████████████████████▋                        | 22/40 [18:58:23<19:27:58, 3893.25s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 22/40 done | last=3173.2s avg=3104.7s elapsed=1138.4m ETA=931.4m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  57%|███████████████████████████████                       | 23/40 [20:03:26<18:23:53, 3896.06s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 23/40 done | last=3902.6s avg=3139.4s elapsed=1203.4m ETA=889.5m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  60%|████████████████████████████████▍                     | 24/40 [20:51:22<15:57:19, 3590.00s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 24/40 done | last=2876.0s avg=3128.4s elapsed=1251.4m ETA=834.2m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  62%|█████████████████████████████████▊                    | 25/40 [21:29:32<13:20:00, 3200.06s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 25/40 done | last=2290.3s avg=3094.9s elapsed=1289.5m ETA=773.7m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  65%|███████████████████████████████████                   | 26/40 [22:30:08<12:57:11, 3330.83s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 26/40 done | last=3635.9s avg=3115.7s elapsed=1350.1m ETA=727.0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  68%|████████████████████████████████████▍                 | 27/40 [23:05:55<10:44:43, 2975.63s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 27/40 done | last=2146.9s avg=3079.8s elapsed=1385.9m ETA=667.3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  70%|█████████████████████████████████████▊                | 28/40 [24:09:39<10:46:02, 3230.22s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 28/40 done | last=3824.2s avg=3106.4s elapsed=1449.7m ETA=621.3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  72%|███████████████████████████████████████▉               | 29/40 [25:02:59<9:50:33, 3221.21s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 29/40 done | last=3200.2s avg=3109.6s elapsed=1503.0m ETA=570.1m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  75%|█████████████████████████████████████████▎             | 30/40 [25:30:29<7:38:18, 2749.83s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 30/40 done | last=1650.0s avg=3061.0s elapsed=1530.5m ETA=510.2m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  78%|██████████████████████████████████████████▋            | 31/40 [25:54:02<5:52:17, 2348.63s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 31/40 done | last=1412.5s avg=3007.8s elapsed=1554.0m ETA=451.2m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  80%|████████████████████████████████████████████           | 32/40 [26:13:56<4:26:58, 2002.34s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 32/40 done | last=1194.3s avg=2951.1s elapsed=1573.9m ETA=393.5m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  82%|█████████████████████████████████████████████▍         | 33/40 [26:33:57<3:25:32, 1761.77s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 33/40 done | last=1200.4s avg=2898.1s elapsed=1593.9m ETA=338.1m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  85%|██████████████████████████████████████████████▊        | 34/40 [26:39:05<2:12:35, 1325.88s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 34/40 done | last=308.8s avg=2821.9s elapsed=1599.1m ETA=282.2m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  88%|████████████████████████████████████████████████▏      | 35/40 [26:45:11<1:26:28, 1037.70s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 35/40 done | last=365.3s avg=2751.7s elapsed=1605.2m ETA=229.3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  90%|████████████████████████████████████████████████████▏     | 36/40 [26:51:22<55:51, 837.95s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 36/40 done | last=371.9s avg=2685.6s elapsed=1611.4m ETA=179.0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  92%|█████████████████████████████████████████████████████▋    | 37/40 [26:57:34<34:53, 697.87s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 37/40 done | last=371.0s avg=2623.1s elapsed=1617.6m ETA=131.2m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  95%|███████████████████████████████████████████████████████   | 38/40 [27:03:45<20:00, 600.06s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 38/40 done | last=371.8s avg=2563.8s elapsed=1623.8m ETA=85.5m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  98%|████████████████████████████████████████████████████████▌ | 39/40 [27:09:50<08:49, 529.29s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 39/40 done | last=364.2s avg=2507.4s elapsed=1629.8m ETA=41.8m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects: 100%|█████████████████████████████████████████████████████████| 40/40 [27:15:46<00:00, 2453.67s/subj]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 40/40 done | last=356.7s avg=2453.7s elapsed=1635.8m ETA=0.0m\n",
      "Shapes -> Left: (40, 0) Right: (40, 0) Concat: (40, 0)\n",
      "Label counts: (array([1]), array([40]))\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "#           FAST TSFRESH PIPELINE (30/15)\n",
    "# =====================================================\n",
    "\n",
    "# Silence numeric warnings from scipy/pandas\n",
    "warnings.filterwarnings(\"ignore\", message=\"Precision loss occurred\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "def sliding_windows(x, win, step):\n",
    "    n = len(x)\n",
    "    if n < win: \n",
    "        return np.empty((0, win))\n",
    "    idx = np.arange(win)[None, :] + np.arange(0, n - win + 1, step)[:, None]\n",
    "    return x[idx]\n",
    "\n",
    "def read_trial(filepath):\n",
    "    \"\"\"\n",
    "    Returns left, right 1D forces. Robust heuristic:\n",
    "    - Take 16 cols (8 per side); choose highest-variance channel per side.\n",
    "    \"\"\"\n",
    "    arr = np.loadtxt(filepath)\n",
    "    if arr.ndim == 1:  # single column -> duplicate\n",
    "        arr = np.stack([arr, arr], axis=1)\n",
    "    ncols = arr.shape[1]\n",
    "    # ensure we can split 8/8; else split in half\n",
    "    k = ncols // 2\n",
    "    L = arr[:, :k]\n",
    "    R = arr[:, k:]\n",
    "    left  = L[:, np.nanargmax(np.nanvar(L, axis=0))]\n",
    "    right = R[:, np.nanargmax(np.nanvar(R, axis=0))]\n",
    "    return left.astype(np.float64), right.astype(np.float64)\n",
    "\n",
    "def windows_to_tsfresh_df(wins, kind):\n",
    "    if wins.size == 0:\n",
    "        return pd.DataFrame(columns=[\"id\",\"time\",\"kind\",\"value\"])\n",
    "    n_win, win_len = wins.shape\n",
    "    return pd.DataFrame({\n",
    "        \"id\":   np.repeat(np.arange(n_win), win_len),\n",
    "        \"time\": np.tile(np.arange(win_len), n_win),\n",
    "        \"value\": wins.ravel(),\n",
    "        \"kind\": kind,\n",
    "    })\n",
    "\n",
    "def subject_feature_row(trial_rows, fs_hz):\n",
    "    WIN  = max(1, round(fs_hz * (WIN_MS/1000)))\n",
    "    STEP = max(1, round(fs_hz * (STEP_MS/1000)))\n",
    "    L_all, R_all = [], []\n",
    "    for path, sid, tid in trial_rows:\n",
    "        left, right = read_trial(path)\n",
    "        L_all.append(sliding_windows(left,  WIN, STEP))\n",
    "        R_all.append(sliding_windows(right, WIN, STEP))\n",
    "    Lw = np.vstack([w for w in L_all if w.size]) if L_all else np.empty((0,WIN))\n",
    "    Rw = np.vstack([w for w in R_all if w.size]) if R_all else np.empty((0,WIN))\n",
    "\n",
    "    Ldf = windows_to_tsfresh_df(Lw, \"L\")\n",
    "    Rdf = windows_to_tsfresh_df(Rw, \"R\")\n",
    "    long_df = pd.concat([Ldf, Rdf], ignore_index=True)\n",
    "    if long_df.empty:\n",
    "        return {}\n",
    "\n",
    "    feats = extract_features(\n",
    "        long_df,\n",
    "        column_id=\"id\", column_sort=\"time\", column_kind=\"kind\", column_value=\"value\",\n",
    "        default_fc_parameters=EfficientFCParameters(),\n",
    "        n_jobs=N_JOBS_TSFRESH, disable_progressbar=True,\n",
    "    )\n",
    "    # Aggregate across windows: mean/std/skew/kurt\n",
    "    agg = pd.DataFrame({\n",
    "        \"mean\": feats.mean(axis=0),\n",
    "        \"std\":  feats.std(axis=0, ddof=1),\n",
    "        \"skew\": feats.apply(pd.Series.skew, axis=0),\n",
    "        \"kurt\": feats.apply(pd.Series.kurt, axis=0),\n",
    "    }).T\n",
    "\n",
    "    out = {}\n",
    "    for c in feats.columns:\n",
    "        base = c.replace(\"value__\", \"\")\n",
    "        kind = \"L\" if \"__L\" in base or \"_L\" in base else (\"R\" if \"__R\" in base or \"_R\" in base else \"\")\n",
    "        key  = base.replace(\"__L\",\"\").replace(\"__R\",\"\").replace(\"_L\",\"\").replace(\"_R\",\"\")\n",
    "        for stat in [\"mean\",\"std\",\"skew\",\"kurt\"]:\n",
    "            out[f\"{kind}_{key}__{stat}\"] = agg.loc[stat, c]\n",
    "    return out\n",
    "\n",
    "# ----------Build per-subject TSFresh----------\n",
    "by_sid = list(trials.groupby(\"subject_id\"))  # materialize for len()\n",
    "rows = []\n",
    "\n",
    "print(f\"TSFresh: extracting features for {len(by_sid)} subjects \"\n",
    "      f\"(win={WIN_MS}ms, step={STEP_MS}ms, fs={FS}Hz, n_jobs={N_JOBS_TSFRESH})\")\n",
    "\n",
    "start_all = time.time()\n",
    "for i, (sid, g) in enumerate(tqdm(by_sid, desc=\"TSFresh subjects\", unit=\"subj\"), 1):\n",
    "    t0 = time.time()\n",
    "    triplets = [(p, sid, tid) for p, tid in zip(g[\"filepath\"], g[\"trial_id\"])]\n",
    "    row = {\"subject_id\": sid}\n",
    "    row.update(subject_feature_row(triplets, FS) or {})\n",
    "    # add label\n",
    "    lab = demo.loc[demo.subject_id == sid, \"label\"]\n",
    "    row[\"label\"] = float(lab.iloc[0]) if not lab.empty else np.nan\n",
    "    rows.append(row)\n",
    "\n",
    "    # live ETA\n",
    "    elapsed = time.time() - start_all\n",
    "    avg_per_subj = elapsed / i\n",
    "    remaining = avg_per_subj * (len(by_sid) - i)\n",
    "    tqdm.write(f\"[TSFresh] {i}/{len(by_sid)} done | \"\n",
    "               f\"last={time.time()-t0:.1f}s avg={avg_per_subj:.1f}s \"\n",
    "               f\"elapsed={elapsed/60:.1f}m ETA={remaining/60:.1f}m\")\n",
    "\n",
    "feat_df = pd.DataFrame(rows).set_index(\"subject_id\").sort_index()\n",
    "feat_df = feat_df.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "left_cols  = [c for c in feat_df.columns if c.startswith(\"L_\")]\n",
    "right_cols = [c for c in feat_df.columns if c.startswith(\"R_\")]\n",
    "\n",
    "X_left  = feat_df[left_cols].copy()\n",
    "X_right = feat_df[right_cols].copy()\n",
    "X_comb  = pd.concat([X_left.add_prefix(\"L|\"), X_right.add_prefix(\"R|\")], axis=1)\n",
    "\n",
    "y = feat_df[\"label\"].astype(int).to_numpy()\n",
    "print(\"Shapes -> Left:\", X_left.shape, \"Right:\", X_right.shape, \"Concat:\", X_comb.shape)\n",
    "print(\"Label counts:\", np.unique(y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f80a7b0-05c1-4daa-8ebc-3458bc4ea925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSFresh: 40 subjects | win=80ms step=40ms fs=100Hz ds=2 n_jobs=14 features=Efficient\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:   2%|█▍                                                        | 1/40 [17:19<11:15:49, 1039.73s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 1/40 | wrote 01.parquet | last=1039.7s avg=1039.7s | elapsed=17.3m ETA=675.8m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:   5%|███                                                         | 2/40 [29:48<9:10:11, 868.72s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 2/40 | wrote 02.parquet | last=749.0s avg=894.4s | elapsed=29.8m ETA=566.4m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:   8%|████▎                                                     | 3/40 [53:51<11:37:20, 1130.84s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 3/40 | wrote 03.parquet | last=1442.8s avg=1077.2s | elapsed=53.9m ETA=664.3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  10%|█████▌                                                  | 4/40 [1:14:26<11:43:06, 1171.85s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 4/40 | wrote 04.parquet | last=1234.7s avg=1116.6s | elapsed=74.4m ETA=669.9m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  12%|███████                                                 | 5/40 [1:34:12<11:26:34, 1176.99s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 5/40 | wrote 05.parquet | last=1186.1s avg=1130.5s | elapsed=94.2m ETA=659.4m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TSFresh subjects:  15%|████████▍                                               | 6/40 [2:03:04<12:53:50, 1365.61s/subj]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSFresh] 6/40 | wrote 06.parquet | last=1731.8s avg=1230.7s | elapsed=123.1m ETA=697.4m\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "#    TSFRESH PIPELINE  — cached, resumable, faster\n",
    "# =====================================================\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from tsfresh.feature_extraction import (\n",
    "    extract_features,\n",
    "    EfficientFCParameters,\n",
    "    MinimalFCParameters,\n",
    ")\n",
    "\n",
    "# ---------- knobs you can tweak ----------\n",
    "WIN_MS, STEP_MS = 80, 40           # wider windows -> fewer windows -> faster & more variance\n",
    "DOWNSAMPLE      = 2                # 1 = off, 2 = take every 2nd sample, etc.\n",
    "USE_MINIMAL_FC  = False            # True -> much faster, fewer features (OK for quick runs)\n",
    "N_JOBS_TSFRESH  = max(1, (os.cpu_count() or 1) - 2)  # leave a little headroom\n",
    "# ----------------------------------------\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Precision loss occurred\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "CACHE_DIR = PROC_DIR / \"tsfresh_cache\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "COMBINED_OUT = PROC_DIR / f\"tsfresh_combined_win{WIN_MS}_step{STEP_MS}_ds{DOWNSAMPLE}.parquet\"\n",
    "\n",
    "fc_params = MinimalFCParameters() if USE_MINIMAL_FC else EfficientFCParameters()\n",
    "\n",
    "def sliding_windows(x, win, step):\n",
    "    n = len(x)\n",
    "    if n < win:\n",
    "        return np.empty((0, win))\n",
    "    idx = np.arange(win)[None, :] + np.arange(0, n - win + 1, step)[:, None]\n",
    "    return x[idx]\n",
    "\n",
    "def read_trial(filepath):\n",
    "    \"\"\"Return (left,right) 1D forces from the .txt file with a robust per-side channel pick.\"\"\"\n",
    "    arr = np.loadtxt(filepath)\n",
    "    if arr.ndim == 1:\n",
    "        arr = np.stack([arr, arr], axis=1)\n",
    "    if DOWNSAMPLE > 1:\n",
    "        arr = arr[::DOWNSAMPLE]\n",
    "\n",
    "    ncols = arr.shape[1]\n",
    "    k = ncols // 2\n",
    "    L = arr[:, :k]\n",
    "    R = arr[:, k:]\n",
    "    left  = L[:, np.nanargmax(np.nanvar(L, axis=0))]\n",
    "    right = R[:, np.nanargmax(np.nanvar(R, axis=0))]\n",
    "    return left.astype(np.float64), right.astype(np.float64)\n",
    "\n",
    "def windows_to_tsfresh_df(wins, kind):\n",
    "    if wins.size == 0:\n",
    "        return pd.DataFrame(columns=[\"id\",\"time\",\"kind\",\"value\"])\n",
    "    n_win, win_len = wins.shape\n",
    "    return pd.DataFrame({\n",
    "        \"id\":   np.repeat(np.arange(n_win), win_len),\n",
    "        \"time\": np.tile(np.arange(win_len), n_win),\n",
    "        \"value\": wins.ravel(),\n",
    "        \"kind\": kind,\n",
    "    })\n",
    "\n",
    "def subject_feature_row(trial_rows, fs_hz):\n",
    "    # effective fs after downsampling\n",
    "    eff_fs = fs_hz / max(1, DOWNSAMPLE)\n",
    "    WIN  = max(1, round(eff_fs * (WIN_MS/1000)))\n",
    "    STEP = max(1, round(eff_fs * (STEP_MS/1000)))\n",
    "\n",
    "    L_all, R_all = [], []\n",
    "    for path, sid, tid in trial_rows:\n",
    "        left, right = read_trial(path)\n",
    "        L_all.append(sliding_windows(left,  WIN, STEP))\n",
    "        R_all.append(sliding_windows(right, WIN, STEP))\n",
    "\n",
    "    Lw = np.vstack([w for w in L_all if w.size]) if L_all else np.empty((0,WIN))\n",
    "    Rw = np.vstack([w for w in R_all if w.size]) if R_all else np.empty((0,WIN))\n",
    "\n",
    "    Ldf = windows_to_tsfresh_df(Lw, \"L\")\n",
    "    Rdf = windows_to_tsfresh_df(Rw, \"R\")\n",
    "    long_df = pd.concat([Ldf, Rdf], ignore_index=True)\n",
    "    if long_df.empty:\n",
    "        return {}\n",
    "\n",
    "    feats = extract_features(\n",
    "        long_df,\n",
    "        column_id=\"id\", column_sort=\"time\", column_kind=\"kind\", column_value=\"value\",\n",
    "        default_fc_parameters=fc_params,\n",
    "        n_jobs=N_JOBS_TSFRESH,\n",
    "        disable_progressbar=True,\n",
    "    )\n",
    "\n",
    "    # aggregate across windows\n",
    "    agg = pd.DataFrame({\n",
    "        \"mean\": feats.mean(axis=0),\n",
    "        \"std\":  feats.std(axis=0, ddof=1),\n",
    "        \"skew\": feats.apply(pd.Series.skew, axis=0),\n",
    "        \"kurt\": feats.apply(pd.Series.kurt, axis=0),\n",
    "    }).T\n",
    "\n",
    "    out = {}\n",
    "    for c in feats.columns:\n",
    "        base = c.replace(\"value__\", \"\")\n",
    "        kind = \"L\" if \"__L\" in base or \"_L\" in base else (\"R\" if \"__R\" in base or \"_R\" in base else \"\")\n",
    "        key  = base.replace(\"__L\",\"\").replace(\"__R\",\"\").replace(\"_L\",\"\").replace(\"_R\",\"\")\n",
    "        for stat in [\"mean\",\"std\",\"skew\",\"kurt\"]:\n",
    "            out[f\"{kind}_{key}__{stat}\"] = agg.loc[stat, c]\n",
    "    return out\n",
    "\n",
    "# --------- cached build per subject ----------\n",
    "by_sid = list(trials.groupby(\"subject_id\"))\n",
    "print(f\"TSFresh: {len(by_sid)} subjects | win={WIN_MS}ms step={STEP_MS}ms \"\n",
    "      f\"fs={FS}Hz ds={DOWNSAMPLE} n_jobs={N_JOBS_TSFRESH} \"\n",
    "      f\"features={'Minimal' if USE_MINIMAL_FC else 'Efficient'}\")\n",
    "\n",
    "start_all = time.time()\n",
    "done, skipped = 0, 0\n",
    "for i, (sid, g) in enumerate(tqdm(by_sid, desc=\"TSFresh subjects\", unit=\"subj\"), 1):\n",
    "    cache_file = CACHE_DIR / f\"{sid}.parquet\"\n",
    "    if cache_file.exists():\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    t0 = time.time()\n",
    "    trips = [(p, sid, tid) for p, tid in zip(g[\"filepath\"], g[\"trial_id\"])]\n",
    "    row = {\"subject_id\": sid}\n",
    "    row.update(subject_feature_row(trips, FS) or {})\n",
    "    # add label\n",
    "    lab = demo.loc[demo.subject_id == sid, \"label\"]\n",
    "    row[\"label\"] = float(lab.iloc[0]) if not lab.empty else np.nan\n",
    "\n",
    "    pd.DataFrame([row]).to_parquet(cache_file, index=False)\n",
    "    done += 1\n",
    "\n",
    "    elapsed = time.time() - start_all\n",
    "    avg = elapsed / max(1, (done + skipped))\n",
    "    remaining = avg * (len(by_sid) - (done + skipped))\n",
    "    tqdm.write(f\"[TSFresh] {i}/{len(by_sid)} | wrote {cache_file.name} \"\n",
    "               f\"| last={time.time()-t0:.1f}s avg={avg:.1f}s \"\n",
    "               f\"| elapsed={elapsed/60:.1f}m ETA={remaining/60:.1f}m\")\n",
    "\n",
    "print(f\"Subjects processed now: {done} | skipped (already cached): {skipped}\")\n",
    "\n",
    "# --------- load from cache, clean, save combined ----------\n",
    "rows = []\n",
    "for f in sorted(CACHE_DIR.glob(\"*.parquet\")):\n",
    "    rows.append(pd.read_parquet(f))\n",
    "if not rows:\n",
    "    raise RuntimeError(\"No cached subject features found. (Did the loop run?)\")\n",
    "\n",
    "feat_df = pd.concat(rows, ignore_index=True).set_index(\"subject_id\").sort_index()\n",
    "feat_df = feat_df.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "# drop constant / all-zero columns\n",
    "before = feat_df.shape\n",
    "feat_df = feat_df.loc[:, feat_df.std(axis=0) > 1e-8]\n",
    "after  = feat_df.shape\n",
    "print(f\"Feature table cleaned: {before} -> {after}\")\n",
    "\n",
    "feat_df.to_parquet(COMBINED_OUT)\n",
    "print(f\"Saved combined features to: {COMBINED_OUT}\")\n",
    "\n",
    "# Split L/R and make matrices\n",
    "left_cols  = [c for c in feat_df.columns if c.startswith(\"L_\")]\n",
    "right_cols = [c for c in feat_df.columns if c.startswith(\"R_\")]\n",
    "\n",
    "X_left  = feat_df[left_cols].copy()\n",
    "X_right = feat_df[right_cols].copy()\n",
    "X_comb  = pd.concat([X_left.add_prefix('L|'), X_right.add_prefix('R|')], axis=1)\n",
    "\n",
    "y = feat_df[\"label\"].astype(int).to_numpy()\n",
    "print(\"Shapes -> Left:\", X_left.shape, \"Right:\", X_right.shape, \"Concat:\", X_comb.shape)\n",
    "print(\"Label counts:\", np.unique(y, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae1958-76b6-4c6b-ae1a-a5a7bcb8120e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e3ac7-28f0-44d0-8902-53c252f9b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "#           FAST TSFRESH PIPELINE (30/15) with CACHE\n",
    "# =====================================================\n",
    "\n",
    "'''import time\n",
    "from tqdm import tqdm\n",
    "from tsfresh.feature_extraction import extract_features, EfficientFCParameters\n",
    "\n",
    "# Silence numeric warnings from scipy/pandas\n",
    "warnings.filterwarnings(\"ignore\", message=\"Precision loss occurred\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "CACHE_DIR = PROC_DIR / \"tsfresh_cache\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def sliding_windows(x, win, step):\n",
    "    n = len(x)\n",
    "    if n < win:\n",
    "        return np.empty((0, win))\n",
    "    idx = np.arange(win)[None, :] + np.arange(0, n - win + 1, step)[:, None]\n",
    "    return x[idx]\n",
    "\n",
    "def read_trial(filepath):\n",
    "    \"\"\"Return left, right 1D forces (pick highest-variance channel per side).\"\"\"\n",
    "    arr = np.loadtxt(filepath)\n",
    "    if arr.ndim == 1:  # single column -> duplicate\n",
    "        arr = np.stack([arr, arr], axis=1)\n",
    "    k = arr.shape[1] // 2\n",
    "    L, R = arr[:, :k], arr[:, k:]\n",
    "    left  = L[:, np.nanargmax(np.nanvar(L, axis=0))]\n",
    "    right = R[:, np.nanargmax(np.nanvar(R, axis=0))]\n",
    "    return left.astype(np.float64), right.astype(np.float64)\n",
    "\n",
    "def windows_to_tsfresh_df(wins, kind):\n",
    "    if wins.size == 0:\n",
    "        return pd.DataFrame(columns=[\"id\",\"time\",\"kind\",\"value\"])\n",
    "    n_win, win_len = wins.shape\n",
    "    return pd.DataFrame({\n",
    "        \"id\":    np.repeat(np.arange(n_win), win_len),\n",
    "        \"time\":  np.tile(np.arange(win_len), n_win),\n",
    "        \"kind\":  kind,\n",
    "        \"value\": wins.ravel(),\n",
    "    })\n",
    "\n",
    "def compute_subject_features(trial_rows, fs_hz):\n",
    "    \"\"\"Compute TSFresh features for one subject (L & R), return a dict.\"\"\"\n",
    "    WIN  = max(1, round(fs_hz * (WIN_MS/1000)))\n",
    "    STEP = max(1, round(fs_hz * (STEP_MS/1000)))\n",
    "\n",
    "    L_all, R_all = [], []\n",
    "    for path, sid, tid in trial_rows:\n",
    "        l, r = read_trial(path)\n",
    "        L_all.append(sliding_windows(l, WIN, STEP))\n",
    "        R_all.append(sliding_windows(r, WIN, STEP))\n",
    "\n",
    "    Lw = np.vstack([w for w in L_all if w.size]) if L_all else np.empty((0, WIN))\n",
    "    Rw = np.vstack([w for w in R_all if w.size]) if R_all else np.empty((0, WIN))\n",
    "\n",
    "    long_df = pd.concat(\n",
    "        [windows_to_tsfresh_df(Lw, \"L\"), windows_to_tsfresh_df(Rw, \"R\")],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    if long_df.empty:\n",
    "        return {}\n",
    "\n",
    "    feats = extract_features(\n",
    "        long_df,\n",
    "        column_id=\"id\", column_sort=\"time\", column_kind=\"kind\", column_value=\"value\",\n",
    "        default_fc_parameters=EfficientFCParameters(),\n",
    "        n_jobs=N_JOBS_TSFRESH,\n",
    "        disable_progressbar=True,\n",
    "    )\n",
    "\n",
    "    # Aggregate across windows: mean/std/skew/kurt per TSFresh column\n",
    "    agg = pd.DataFrame({\n",
    "        \"mean\": feats.mean(axis=0),\n",
    "        \"std\":  feats.std(axis=0, ddof=1),\n",
    "        \"skew\": feats.apply(pd.Series.skew, axis=0),\n",
    "        \"kurt\": feats.apply(pd.Series.kurt, axis=0),\n",
    "    }).T\n",
    "\n",
    "    out = {}\n",
    "    for c in feats.columns:\n",
    "        base = c.replace(\"value__\", \"\")\n",
    "        kind = \"L\" if (\"__L\" in base or \"_L\" in base) else (\"R\" if (\"__R\" in base or \"_R\" in base) else \"\")\n",
    "        key  = base.replace(\"__L\",\"\").replace(\"__R\",\"\").replace(\"_L\",\"\").replace(\"_R\",\"\")\n",
    "        for stat in [\"mean\",\"std\",\"skew\",\"kurt\"]:\n",
    "            out[f\"{kind}_{key}__{stat}\"] = agg.loc[stat, c]\n",
    "    return out\n",
    "\n",
    "# ---------- Build per-subject TSFresh with caching ----------\n",
    "by_sid = list(trials.groupby(\"subject_id\"))\n",
    "print(f\"TSFresh: extracting features for {len(by_sid)} subjects \"\n",
    "      f\"(win={WIN_MS}ms, step={STEP_MS}ms, fs={FS}Hz, n_jobs={N_JOBS_TSFRESH})\")\n",
    "\n",
    "rows = []\n",
    "start_all = time.time()\n",
    "\n",
    "for i, (sid, g) in enumerate(tqdm(by_sid, desc=\"TSFresh subjects\", unit=\"subj\"), 1):\n",
    "    cache_file = CACHE_DIR / f\"subject_{sid}.parquet\"\n",
    "\n",
    "    t0 = time.time()\n",
    "    if cache_file.exists():\n",
    "        # resume: load cached\n",
    "        row = pd.read_parquet(cache_file).to_dict(orient=\"records\")[0]\n",
    "    else:\n",
    "        triplets = [(p, sid, tid) for p, tid in zip(g[\"filepath\"], g[\"trial_id\"])]\n",
    "        row = {\"subject_id\": sid}\n",
    "        row.update(compute_subject_features(triplets, FS) or {})\n",
    "        # add label\n",
    "        lab = demo.loc[demo.subject_id == sid, \"label\"]\n",
    "        row[\"label\"] = float(lab.iloc[0]) if not lab.empty else np.nan\n",
    "        # cache immediately\n",
    "        pd.DataFrame([row]).to_parquet(cache_file, index=False)\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "    # live ETA print\n",
    "    elapsed = time.time() - start_all\n",
    "    avg = elapsed / i\n",
    "    remain = avg * (len(by_sid) - i)\n",
    "    tqdm.write(f\"[TSFresh] {i}/{len(by_sid)} done | last={time.time()-t0:.1f}s \"\n",
    "               f\"avg={avg:.1f}s elapsed={elapsed/60:.1f}m ETA={remain/60:.1f}m\")\n",
    "\n",
    "# ---------- Assemble full feature tables ----------\n",
    "feat_df = pd.DataFrame(rows).set_index(\"subject_id\").sort_index()\n",
    "feat_df = feat_df.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "left_cols  = [c for c in feat_df.columns if c.startswith(\"L_\")]\n",
    "right_cols = [c for c in feat_df.columns if c.startswith(\"R_\")]\n",
    "\n",
    "X_left  = feat_df[left_cols].copy()\n",
    "X_right = feat_df[right_cols].copy()\n",
    "X_comb  = pd.concat([X_left.add_prefix(\"L|\"), X_right.add_prefix(\"R|\")], axis=1)\n",
    "\n",
    "# save combined artifacts (so you can skip TSFresh entirely next time)\n",
    "( PROC_DIR / \"tsfresh_left.parquet\"        ).write_bytes(X_left.to_parquet(index=True))\n",
    "( PROC_DIR / \"tsfresh_right.parquet\"       ).write_bytes(X_right.to_parquet(index=True))\n",
    "( PROC_DIR / \"tsfresh_combined_concat.parquet\" ).write_bytes(X_comb.to_parquet(index=True))\n",
    "( PROC_DIR / \"tsfresh_labels.parquet\"      ).write_bytes(pd.Series(\n",
    "    feat_df[\"label\"].astype(int), name=\"label\").to_frame().to_parquet(index=True))\n",
    "\n",
    "# labels array for plots & baselines\n",
    "y = feat_df[\"label\"].astype(int).to_numpy()\n",
    "print(\"Shapes -> Left:\", X_left.shape, \"Right:\", X_right.shape, \"Concat:\", X_comb.shape)\n",
    "print(\"Label counts:\", np.unique(y, return_counts=True))\n",
    "\n",
    "# ========= 5) PCA / t-SNE (z-score first) =========\n",
    "def plot_dr(X, y, title, method=\"pca\", seed=SEED, save=None):\n",
    "    Xs = StandardScaler().fit_transform(X.values)\n",
    "    if method == \"pca\":\n",
    "        Z = PCA(n_components=2, random_state=seed).fit_transform(Xs)\n",
    "    else:\n",
    "        Z = TSNE(n_components=2, perplexity=10, n_iter=1000, learning_rate=\"auto\",\n",
    "                 init=\"pca\", random_state=seed).fit_transform(Xs)\n",
    "    plt.figure(figsize=(7,5.5))\n",
    "    for lab, m in [(0, \"o\"), (1, \"^\")]:\n",
    "        idx = (y == lab)\n",
    "        plt.scatter(Z[idx,0], Z[idx,1], marker=m, alpha=0.9,\n",
    "                    label=\"Control\" if lab==0 else \"PD\")\n",
    "    plt.legend(); plt.title(title); plt.xlabel(\"dim 1\"); plt.ylabel(\"dim 2\"); plt.tight_layout()\n",
    "    if save: plt.savefig(FIG_DIR / save, dpi=160)  # << save to FIG_DIR\n",
    "    plt.show()\n",
    "\n",
    "plot_dr(X_right, y, \"PCA – Right features (TSFresh)\", method=\"pca\", save=\"PCA_Right_features_TSFresh.png\")\n",
    "plot_dr(X_left,  y, \"PCA – Left features (TSFresh)\",  method=\"pca\", save=\"PCA_Left_features_TSFresh.png\")\n",
    "plot_dr(X_comb,  y, \"PCA – Combined L|R (concat)\",    method=\"pca\", save=\"PCA_Combined_L_R_concat.png\")\n",
    "# plot_dr(X_comb,  y, \"tSNE – Combined L|R (concat)\", method=\"tsne\", save=\"tSNE_Combined_L_R_concat.png\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "098fe228-1d36-4c1a-80bf-49558fd635ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(40, 0)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save: plt\u001b[38;5;241m.\u001b[39msavefig(REPORT_DIR \u001b[38;5;241m/\u001b[39m save, dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m160\u001b[39m)\n\u001b[0;32m     15\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 17\u001b[0m \u001b[43mplot_dr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_right\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPCA – Right features (TSFresh)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpca\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPCA_Right_features_TSFresh.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m plot_dr(X_left,  y, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA – Left features (TSFresh)\u001b[39m\u001b[38;5;124m\"\u001b[39m,  method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca\u001b[39m\u001b[38;5;124m\"\u001b[39m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA_Left_features_TSFresh.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m plot_dr(X_comb,  y, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA – Combined L|R (concat)\u001b[39m\u001b[38;5;124m\"\u001b[39m,   method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca\u001b[39m\u001b[38;5;124m\"\u001b[39m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA_Combined_L_R_concat.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m, in \u001b[0;36mplot_dr\u001b[1;34m(X, y, title, method, seed, save)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot_dr\u001b[39m(X, y, title, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca\u001b[39m\u001b[38;5;124m\"\u001b[39m, seed\u001b[38;5;241m=\u001b[39mSEED, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m----> 3\u001b[0m     Xs \u001b[38;5;241m=\u001b[39m \u001b[43mStandardScaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      5\u001b[0m         Z \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mseed)\u001b[38;5;241m.\u001b[39mfit_transform(Xs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gait_mamba\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gait_mamba\\lib\\site-packages\\sklearn\\base.py:894\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    879\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    880\u001b[0m             (\n\u001b[0;32m    881\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    889\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    890\u001b[0m         )\n\u001b[0;32m    892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    895\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gait_mamba\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:907\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 907\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gait_mamba\\lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gait_mamba\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:943\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \n\u001b[0;32m    913\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    942\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 943\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gait_mamba\\lib\\site-packages\\sklearn\\utils\\validation.py:2954\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2952\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2953\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2954\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2955\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2956\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\gait_mamba\\lib\\site-packages\\sklearn\\utils\\validation.py:1137\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m<\u001b[39m ensure_min_features:\n\u001b[1;32m-> 1137\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1138\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m feature(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1139\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m a minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1140\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_features, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_features, context)\n\u001b[0;32m   1141\u001b[0m         )\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_non_negative:\n\u001b[0;32m   1144\u001b[0m     whom \u001b[38;5;241m=\u001b[39m input_name\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(40, 0)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "# ========= 5) PCA / t-SNE (z-score first) =========\n",
    "def plot_dr(X, y, title, method=\"pca\", seed=SEED, save=None):\n",
    "    Xs = StandardScaler().fit_transform(X.values)\n",
    "    if method == \"pca\":\n",
    "        Z = PCA(n_components=2, random_state=seed).fit_transform(Xs)\n",
    "    else:\n",
    "        Z = TSNE(n_components=2, perplexity=10, n_iter=1000, learning_rate=\"auto\",\n",
    "                 init=\"pca\", random_state=seed).fit_transform(Xs)\n",
    "    plt.figure(figsize=(7,5.5))\n",
    "    for lab, m, c in [(0, \"o\", \"#3b6fb6\"), (1, \"^\", \"#f39c12\")]:\n",
    "        idx = (y==lab)\n",
    "        plt.scatter(Z[idx,0], Z[idx,1], marker=m, alpha=0.9, label=\"Control\" if lab==0 else \"PD\")\n",
    "    plt.legend(); plt.title(title); plt.xlabel(\"dim 1\"); plt.ylabel(\"dim 2\"); plt.tight_layout()\n",
    "    if save: plt.savefig(REPORT_DIR / save, dpi=160)\n",
    "    plt.show()\n",
    "\n",
    "plot_dr(X_right, y, \"PCA – Right features (TSFresh)\", method=\"pca\", save=\"PCA_Right_features_TSFresh.png\")\n",
    "plot_dr(X_left,  y, \"PCA – Left features (TSFresh)\",  method=\"pca\", save=\"PCA_Left_features_TSFresh.png\")\n",
    "plot_dr(X_comb,  y, \"PCA – Combined L|R (concat)\",   method=\"pca\", save=\"PCA_Combined_L_R_concat.png\")\n",
    "# If you want one t-SNE only:\n",
    "# plot_dr(X_comb,  y, \"tSNE – Combined L|R (concat)\", method=\"tsne\", save=\"tSNE_Combined_L_R_concat.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a1a24-501c-4104-953e-aa4b82e2268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "#                 BASELINE C: MiniRocket\n",
    "# =====================================================\n",
    "\n",
    "from sktime.transformations.panel.rocket import MiniRocketMultivariate\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def subject_series_LR(trial_rows, target_len=8000):\n",
    "    Ls, Rs = [], []\n",
    "    for path, sid, tid in trial_rows:\n",
    "        l, r = read_trial(path)\n",
    "        Ls.append(l); Rs.append(r)\n",
    "    L = np.concatenate(Ls) if Ls else np.zeros(1)\n",
    "    R = np.concatenate(Rs) if Rs else np.zeros(1)\n",
    "    # per-channel standardization\n",
    "    L = (L - L.mean()) / (L.std() + 1e-8)\n",
    "    R = (R - R.mean()) / (R.std() + 1e-8)\n",
    "    def fix_len(x):\n",
    "        if len(x) >= target_len:\n",
    "            return x[:target_len]\n",
    "        out = np.zeros(target_len); out[:len(x)] = x; return out\n",
    "    return np.vstack([fix_len(L), fix_len(R)])  # (2, T)\n",
    "\n",
    "# Build nested DataFrame (with progress)\n",
    "X_list, y_list = [], []\n",
    "subs = list(trials.groupby(\"subject_id\"))\n",
    "print(f\"MiniRocket: building nested data for {len(subs)} subjects …\")\n",
    "for sid, g in tqdm(subs, desc=\"Build nested\", unit=\"subj\"):\n",
    "    trips = [(p, sid, tid) for p, tid in zip(g[\"filepath\"], g[\"trial_id\"])]\n",
    "    arr2 = subject_series_LR(trips, target_len=8000)  # you can lower to 4000 for speed\n",
    "    X_list.append(arr2)\n",
    "    lab = demo.loc[demo.subject_id == sid, \"label\"]\n",
    "    y_list.append(int(lab.iloc[0]) if not lab.empty else 0)\n",
    "\n",
    "def to_nested(X_list):\n",
    "    C = X_list[0].shape[0]\n",
    "    cols = {}\n",
    "    for c in range(C):\n",
    "        cols[f\"ch{c}\"] = pd.Series([pd.Series(x[c]) for x in X_list], dtype=\"object\")\n",
    "    return pd.DataFrame(cols)\n",
    "\n",
    "Xn = to_nested(X_list)\n",
    "y_mr = np.array(y_list, dtype=int)\n",
    "print(\"MiniRocket data:\", Xn.shape, \"labels:\", np.bincount(y_mr))\n",
    "\n",
    "# You can lower num_kernels for faster runs (e.g., 3000 or 2000)\n",
    "mrmv = MiniRocketMultivariate(num_kernels=5000, random_state=SEED)\n",
    "clf  = RidgeClassifierCV(alphas=np.logspace(-3, 3, 7))\n",
    "pipe = make_pipeline(mrmv, clf)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "scores = []\n",
    "start_cv = time.time()\n",
    "for k, (tr, te) in enumerate(cv.split(Xn, y_mr), 1):\n",
    "    fold_t0 = time.time()\n",
    "    pipe.fit(Xn.iloc[tr], y_mr[tr])\n",
    "    acc = pipe.score(Xn.iloc[te], y_mr[te])\n",
    "    scores.append(acc)\n",
    "\n",
    "    # timing + ETA\n",
    "    elapsed = time.time() - start_cv\n",
    "    avg_per_fold = elapsed / k\n",
    "    remaining = avg_per_fold * (5 - k)\n",
    "    print(f\"[MiniRocket] fold {k}/5 acc={acc:.3f} | \"\n",
    "          f\"fold_time={time.time()-fold_t0:.1f}s | \"\n",
    "          f\"elapsed={elapsed/60:.1f}m ETA={remaining/60:.1f}m\")\n",
    "\n",
    "scores = np.array(scores)\n",
    "print(f\"MiniRocket 5-fold accuracy: {scores.mean():.3f} ± {scores.std():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gait_mamba)",
   "language": "python",
   "name": "gait_mamba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
